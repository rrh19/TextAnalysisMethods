{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types and counts of stories\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "type\n",
       "bias            443\n",
       "bs            11492\n",
       "conspiracy      430\n",
       "fake             19\n",
       "hate            246\n",
       "junksci         102\n",
       "satire          146\n",
       "state           121\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Types and counts of stories\")\n",
    "df = frame.groupby([\"type\"]).size()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#from pandas import DataFrame, Series\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "frame = pd.read_csv(\"fake.csv\")\n",
    "real_frame = pd.read_csv(\"all-the-news/articles1.csv\")\n",
    "frame.drop(['uuid', 'ord_in_thread','published','language','crawled'], axis=1, inplace=True)\n",
    "real_frame.drop(['id', 'publication', 'author', 'date', 'year', 'month', 'url', 'content'], axis=1, inplace=True)\n",
    "real_frame.columns = ['index', 'titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_f = frame.fillna('No data in title field')\n",
    "df_r = real_frame.fillna('No data in title field')\n",
    "\n",
    "shape = df_f.shape\n",
    "fake_category = [0] * shape[0]\n",
    "\n",
    "shape = df_r.shape\n",
    "real_category = [1] * shape[0]\n",
    "\n",
    "df_f = df_f.filter(items=['text'])\n",
    "df_r = real_frame.filter(items=['titles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#this variable is a list of titles, but some of the titles are strings and some are \"nan\" values from the dataframe. \n",
    "#filter out of the nan values to get the code below to work\n",
    "fake_titles = df_f['text'].tolist()\n",
    "real_titles = df_r['titles'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests \n",
    "import enchant\n",
    "\n",
    "words = requests.get('http://ir.dcs.gla.ac.uk/resources/linguistic_utils/stop_words')\n",
    "stoplist1 = words.text.split(\"\\r\\n\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stoplist2 = set(stopwords.words('english'))\n",
    "\n",
    "stoplist1.extend(stoplist2)\n",
    "\n",
    "fullstops = list(set(stoplist1))\n",
    "\n",
    "def remove_stops(stoplist, wordlist):\n",
    "    result = []\n",
    "    for i in wordlist:\n",
    "        if i not in stoplist:\n",
    "                result.append(i)\n",
    "    return result\n",
    "\n",
    "def spellcheck(wordlist):\n",
    "    result = []\n",
    "    d = enchant.Dict(\"en_US\")\n",
    "    for i in wordlist:\n",
    "        if d.check(i) or d.check(i.capitalize()):\n",
    "            result.append(i)\n",
    "         \n",
    "    return result\n",
    "def clean_text(list_of_texts):\n",
    "    fully_cleaned =[]\n",
    "    #normalize ocr errors\n",
    "    for i in list_of_texts:\n",
    "        #lowercase all\n",
    "        ocr_lower = i.lower()\n",
    "        #tokenize, remove punctuation and numbers, remove tabs, newlines, etc.\n",
    "        ocr_cleaner = ocr_lower.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "        ocr_tokens = ocr_cleaner.split(\" \")\n",
    "        \n",
    "        no_numbers_or_punct = []\n",
    "        for token in ocr_tokens:\n",
    "            if token.isalpha():\n",
    "                no_numbers_or_punct.append(token)\n",
    "            else:\n",
    "                \n",
    "                new_token = \"\"\n",
    "                for letter in token:\n",
    "                    if letter.isalpha():\n",
    "                        new_token += letter\n",
    "                if new_token != \"\":\n",
    "                    no_numbers_or_punct.append(new_token)  \n",
    "        #almost_ready_before_spellcheck = remove_stops(fullstops, no_numbers_or_punct)\n",
    "        #almost_ready = spellcheck(almost_ready_before_spellcheck)\n",
    "        #from nltk.stem import WordNetLemmatizer\n",
    "        #lemmatization\n",
    "        #wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        #lemmas = []\n",
    "        #for token in almost_ready:\n",
    "        #    lemma = wordnet_lemmatizer.lemmatize(token)\n",
    "        #    lemmas.append(lemma)\n",
    "        \n",
    "        #ready = [i for i in lemmas if len(i) > 2]\n",
    "        #fully_cleaned.append(ready)\n",
    "        fully_cleaned.append(no_numbers_or_punct)\n",
    "    return fully_cleaned\n",
    "ocr_cleaned_fake = clean_text(fake_titles)\n",
    "ocr_cleaned_real = clean_text(real_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "ocr_counters_fake = [Counter(i) for i in ocr_cleaned_fake]\n",
    "ocr_counters_real = [Counter(i) for i in ocr_cleaned_real]\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "v = DictVectorizer()\n",
    "x_f = v.fit_transform(ocr_counters_fake)\n",
    "x_r = v.fit_transform(ocr_counters_real)\n",
    "y = TfidfTransformer()\n",
    "z_f = y.fit_transform(x_f)\n",
    "z_r = y.fit_transform(x_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "scaled_vsm_f = z_f.toarray()\n",
    "scaled_vsm_r = z_r.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
